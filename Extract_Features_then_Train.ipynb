{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.densenet import DenseNet121, DenseNet201\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2, os, os, git, glob, random\n",
    "import numpy as np\n",
    "from insectrec.utils import get_dataset, train_generator, valid_generator, augment_trainset\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "repo = git.Repo('.', search_parent_directories=True)\n",
    "created_data_path = f'{repo.working_tree_dir}/insectrec/created_data'\n",
    "path_impy_crops_export = f'{created_data_path}/impy_crops_export/'\n",
    "path_images_augmented = f'{created_data_path}/images_augmented/'\n",
    "\n",
    "if not os.path.isdir(path_images_augmented):\n",
    "    raise NotImplemented(\"Not expanded dataset yet!\")\n",
    "\n",
    "batch_size = 128\n",
    "img_dim = 90\n",
    "modelname = 'xtract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating le for encoding labels\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Creating dataframe with all the original data (x: filenames, textlabels, y: nummerical labels)\n",
    "df_orig = pd.DataFrame()\n",
    "df_orig['x'] = pd.Series(glob.glob(f\"{path_impy_crops_export}/*/*.jpg\"))\n",
    "df_orig['y_text'] = df_orig['x'].apply(lambda x: x.split('/')[-2])\n",
    "df_orig['y'] = le.fit_transform(df_orig.y_text)\n",
    "\n",
    "# Splitting into train/val/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_orig.x, df_orig.y, test_size=0.2, random_state=seed, shuffle=True)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=seed, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering info on augmented X_train data\n",
    "df_aug = pd.DataFrame()\n",
    "df_aug['x'] = pd.Series(glob.glob(f\"{path_images_augmented}/*/*.jpg\"))\n",
    "df_aug['textlabels'] = df_aug['x'].apply(lambda x: x.split('/')[-2])\n",
    "df_aug['y'] = le.fit_transform(df_aug.textlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_aug = df_aug.x.tolist()\n",
    "y_train_aug = df_aug.y.tolist()\n",
    "\n",
    "c = list(zip(X_train_aug, y_train_aug))\n",
    "random.shuffle(c)\n",
    "X_train_aug, y_train_aug = zip(*c)\n",
    "X_train_aug = list(X_train_aug)\n",
    "y_train_aug = list(y_train_aug)\n",
    "X_val = X_val.tolist()\n",
    "y_val = y_val.tolist()\n",
    "X_test = X_test.tolist()\n",
    "y_test = y_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_average_pooling2d_4/Identity:0' shape=(None, 1024) dtype=float32>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = DenseNet121(include_top=False, \n",
    "                         weights='imagenet', \n",
    "                        input_shape=(img_dim,img_dim,3)) \n",
    "x = base_model.output\n",
    "predictions = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(128, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# predictions = Dense(6, activation='relu')(x)\n",
    "# predictions = Dense(6, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.layers[-1].output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features for our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380/379 [==============================] - 27s 71ms/step\n"
     ]
    }
   ],
   "source": [
    "X_pred_aug = model.predict(valid_generator(X_train_aug, y_train_aug, batch_size=batch_size, nb_classes=6, img_dim=img_dim), \n",
    "                         steps= len(X_train_aug) / batch_size, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48624, 1024)\n"
     ]
    }
   ],
   "source": [
    "print(X_pred_aug.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using extracted features with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.metrics import confusion_matrix, balanced_accuracy_score, classification_report, make_scorer, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed: 327.5min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed: 1838.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV(cv=5, error_score=nan,\n",
      "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "                                     colsample_bylevel=1, colsample_bynode=1,\n",
      "                                     colsample_bytree=1, gamma=0,\n",
      "                                     learning_rate=0.1, max_delta_step=0,\n",
      "                                     max_depth=3, min_child_weight=1,\n",
      "                                     missing=None, n_estimators=100, n_jobs=1,\n",
      "                                     nthread=None, objective='binary:logistic',\n",
      "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "                                     scale_pos_weight=1, seed=None, silent=None,\n",
      "                                     subsample=1, verbosity=1),\n",
      "             iid='deprecated', n_jobs=-1,\n",
      "             param_grid={'gamma': [0.1, 0], 'learning_rate': [0.2, 0.3, 0.4],\n",
      "                         'n_estimators': [500, 750, 1000], 'n_jobs': [-1],\n",
      "                         'nthread': [8]},\n",
      "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
      "             scoring=make_scorer(balanced_accuracy_score), verbose=2)\n"
     ]
    }
   ],
   "source": [
    "parameters = {'nthread': [8],\n",
    "              'gamma': [0.1, 0], \n",
    "              'learning_rate': [0.2, 0.3, 0.4], #so called `eta` value,\n",
    "              'n_estimators': [500, 750, 1000],\n",
    "             'n_jobs': [-1]}\n",
    "\n",
    "gridsearch = GridSearchCV(XGBClassifier(), \n",
    "                   parameters, \n",
    "                   n_jobs=-1, \n",
    "                   cv=5, \n",
    "                   scoring=make_scorer(balanced_accuracy_score),\n",
    "                   verbose=2, \n",
    "                   refit=True)\n",
    "\n",
    "gridsearch.fit(X_pred_aug, y_train_aug)\n",
    "\n",
    "print(gridsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
       "              learning_rate=0.4, max_delta_step=0, max_depth=3,\n",
       "              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=-1,\n",
       "              nthread=8, objective='multi:softprob', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_estimator = gridsearch.best_estimator_\n",
    "final_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_results = cross_validate(final_estimator, X_pred_aug, y_train_aug, cv=5, \n",
    "#                             return_estimator=True, \n",
    "#                             return_train_score=True, \n",
    "#                             scoring=make_scorer(balanced_accuracy_score),\n",
    "#                             verbose=1, \n",
    "#                             n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=1, gamma=0,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=3, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'gamma': [0.1, 0], 'learning_rate': [0.2, 0.3, 0.4],\n",
       "                         'n_estimators': [500, 750, 1000], 'n_jobs': [-1],\n",
       "                         'nthread': [8]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(balanced_accuracy_score), verbose=2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting test features for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/20 [===============================] - 2s 87ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test_pred = model.predict(valid_generator(X_test, y_test, batch_size=batch_size, nb_classes=6, img_dim=img_dim), \n",
    "                         steps= len(X_test) / batch_size, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6479034652026391"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_test,final_estimator.predict(X_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a relu size 6 in the end\n",
    "## include_top, untrained 0.3344455669500479\n",
    "# using global avg pooling in the end 0.6479034652026391\n",
    "## include_top, imagenet  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('objdetect': conda)",
   "language": "python",
   "name": "python37664bitobjdetectconda73730bfb74724935b9013e8a9503c070"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
