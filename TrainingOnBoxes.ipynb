{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# from pyimagesearch.lenet import LeNet\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# initialize the number of epochs to train for, initial learning rate,\n",
    "# and batch size\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "monitor='val_loss'\n",
    "es_patience=7\n",
    "rlr_patience=3\n",
    "img_width, img_height = 60,60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " loading images...\n",
      " loading images...\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(dataset='./insectrec/created_data/impy_crops_export/'):\n",
    "    # initialize the data and labels\n",
    "    print(\" loading images...\")\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # grab the image paths and randomly shuffle them\n",
    "    imagePaths = sorted(list(paths.list_images(dataset)))\n",
    "    random.seed(42)\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    # loop over the input images\n",
    "    for imagePath in imagePaths:\n",
    "        # load the image, pre-process it, and store it in the data list\n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.resize(image, (img_width, img_height))\n",
    "        image = img_to_array(image)\n",
    "        data.append(image)\n",
    "\n",
    "        # extract the class label from the image path and update the\n",
    "        # labels list\n",
    "        label = imagePath.split(os.path.sep)[-2]\n",
    "        labels.append(label)\n",
    "\n",
    "    # scale the raw pixel intensities to the range [0, 1]\n",
    "    data = np.array(data, dtype=\"float\") / 255.0\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    # partition the data into training and testing splits using 75% of\n",
    "    # the data for training and the remaining 25% for testing\n",
    "    (trainX, testX, trainY, testY) = train_test_split(data,\n",
    "        labels, test_size=0.2, random_state=42)\n",
    "    return trainX, testX, trainY, testY, labels\n",
    "\n",
    "_, testX, _, testY, _ = get_dataset(dataset='./insectrec/created_data/impy_crops_export/')\n",
    "trainX, _, trainY, _, labels = get_dataset(dataset='./insectrec/created_data/images_augmented/')\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = to_categorical(trainY, num_classes=6)\n",
    "testY = to_categorical(testY, num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, zoom_range=0.3,\n",
    "    horizontal_flip=True, vertical_flip=True,fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.densenet import DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "log_dir = './insectrec/created_data/logs/'\n",
    "top_weights_path = f'./insectrec/created_data/weights/model_DenseNet_{img_width}.h5'\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(monitor =  monitor,\n",
    "                                  filepath =  top_weights_path,\n",
    "                                  save_best_only = False,\n",
    "                                  save_weights_only = False,\n",
    "                                  verbose = 1),\n",
    "                  EarlyStopping(monitor =  monitor,\n",
    "                                patience =  es_patience,\n",
    "                                verbose = 1),\n",
    "                  ReduceLROnPlateau(monitor =  monitor,\n",
    "                                    factor = 0.1,\n",
    "                                    patience =  rlr_patience,\n",
    "                                    verbose = 1),\n",
    "                # CSVLogger(filename =  logfile),\n",
    "                  TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = DenseNet121(include_top=True, weights=None, \n",
    "                         input_shape=(img_width,img_height,3))\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "# x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1909/1910 [============================>.] - ETA: 1s - loss: 1.4243 - accuracy: 0.4266\n",
      "Epoch 00001: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "1910/1910 [==============================] - 1950s 1s/step - loss: 1.4242 - accuracy: 0.4266 - val_loss: 1.3683 - val_accuracy: 0.4522\n",
      "Epoch 2/100\n",
      "1909/1910 [============================>.] - ETA: 1s - loss: 1.2277 - accuracy: 0.5243\n",
      "Epoch 00002: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "1910/1910 [==============================] - 2015s 1s/step - loss: 1.2275 - accuracy: 0.5243 - val_loss: 1.4611 - val_accuracy: 0.4434\n",
      "Epoch 3/100\n",
      "1909/1910 [============================>.] - ETA: 0s - loss: 1.1269 - accuracy: 0.5703\n",
      "Epoch 00003: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "1910/1910 [==============================] - 1932s 1s/step - loss: 1.1270 - accuracy: 0.5703 - val_loss: 1.0850 - val_accuracy: 0.6001\n",
      "Epoch 4/100\n",
      "1909/1910 [============================>.] - ETA: 0s - loss: 1.0958 - accuracy: 0.5801\n",
      "Epoch 00004: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "1910/1910 [==============================] - 1912s 1s/step - loss: 1.0957 - accuracy: 0.5801 - val_loss: 1.7931 - val_accuracy: 0.3754\n",
      "Epoch 5/100\n",
      "1909/1910 [============================>.] - ETA: 1s - loss: 1.0474 - accuracy: 0.6000\n",
      "Epoch 00005: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "1910/1910 [==============================] - 1957s 1s/step - loss: 1.0474 - accuracy: 0.6000 - val_loss: 1.3901 - val_accuracy: 0.4818\n",
      "Epoch 6/100\n",
      "1909/1910 [============================>.] - ETA: 0s - loss: 1.0223 - accuracy: 0.6120\n",
      "Epoch 00006: saving model to ./insectrec/created_data/weights/model_DenseNet_60.h5\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "1910/1910 [==============================] - 1930s 1s/step - loss: 1.0222 - accuracy: 0.6120 - val_loss: 1.2266 - val_accuracy: 0.5757\n",
      "Epoch 7/100\n",
      "  25/1910 [..............................] - ETA: 31:55 - loss: 1.0146 - accuracy: 0.6150"
     ]
    }
   ],
   "source": [
    "FH = model.fit_generator(aug.flow(trainX, trainY, batch_size=batch_size),\n",
    "    validation_data=(testX, testY), steps_per_epoch=len(trainX) // batch_size,\n",
    "    epochs=epochs, verbose=1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
